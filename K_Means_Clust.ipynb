{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering for Customer Segmentation\n",
    "\n",
    "This notebook demonstrates the application of the K-Means clustering algorithm for customer segmentation. We will explore a synthetic customer dataset, preprocess the data, train the model, evaluate the results, and derive insights on customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "K-Means clustering is an unsupervised machine learning algorithm used to partition data into distinct clusters. Its primary objective is to minimize the within-cluster variance, thus grouping similar data points together. \n",
    "\n",
    "**Significance and Real-World Applications:**\n",
    "\n",
    "- **Customer Segmentation:** Helps businesses tailor marketing strategies based on customer purchasing behavior and demographics.\n",
    "- **Image Segmentation:** Used for partitioning images into meaningful segments for object detection or medical imaging.\n",
    "- **Gene Expression Analysis:** Aids in identifying patterns in gene expression for biological research.\n",
    "\n",
    "In this notebook, we focus on customer segmentation to understand different customer groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description & Exploratory Analysis\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "For demonstration purposes, we use a synthetic customer dataset with the following features:\n",
    "\n",
    "- **CustomerID:** Unique identifier for each customer.\n",
    "- **Age:** Age of the customer.\n",
    "- **Annual Income (k$):** Annual income in thousands of dollars.\n",
    "- **Spending Score (1-100):** A score assigned by the mall based on customer behavior and spending nature.\n",
    "\n",
    "The dataset is simulated to resemble common customer segmentation problems seen in retail analytics.\n",
    "\n",
    "### Exploratory Analysis\n",
    "\n",
    "We will perform an initial exploratory data analysis (EDA) to understand the distribution and relationships of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic customer dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "data = {\n",
    "    \"CustomerID\": np.arange(1, n_samples + 1),\n",
    "    \"Age\": np.random.randint(18, 70, size=n_samples),\n",
    "    \"Annual Income (k$)\": np.random.randint(15, 140, size=n_samples),\n",
    "    \"Spending Score (1-100)\": np.random.randint(1, 101, size=n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploratory analysis\n",
    "print(\"Dataset Summary:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Info:\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships between features\n",
    "sns.pairplot(df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']], diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "1. **Handling Missing Values:** Check and impute or drop missing values. (Our synthetic data does not have missing values.)\n",
    "2. **Feature Scaling:** Standardize features to bring them to the same scale. This is important for distance-based algorithms like K-Means.\n",
    "3. **Dimensionality Reduction (if necessary):** In cases of high-dimensional data, techniques like PCA may be applied. In this project, we work with three features, so this step is not required.\n",
    "\n",
    "Let's proceed with scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values in each column:\\n', df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop CustomerID as it's not relevant for clustering\n",
    "df_clean = df.drop('CustomerID', axis=1)\n",
    "\n",
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_clean)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_clean.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation\n",
    "\n",
    "The goal of K-Means is to partition the dataset into **K** clusters such that the within-cluster sum of squares is minimized.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The objective function is defined as:\n",
    "\n",
    "$J = \\sum_{i=1}^{K} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$\n",
    "\n",
    "where:\n",
    "\n",
    "- **$K$**: Number of clusters\n",
    "- **$C$<sub>i</sub>**: Set of points in cluster *i*\n",
    "- **$\\mu_i$**: Centroid of cluster *i*\n",
    "\n",
    "### Distance Metric\n",
    "\n",
    "K-Means uses the Euclidean distance to measure the similarity between points:\n",
    "\n",
    "$d(x, \\mu) = \\sqrt{\\sum_{j=1}^{n} (x_j - \\mu_j)^2}$\n",
    "\n",
    "### Centroid Update\n",
    "\n",
    "At each iteration, the centroid of each cluster is updated as the mean of all points assigned to that cluster:\n",
    "\n",
    "$\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$\n",
    "\n",
    "### Convergence Criteria\n",
    "\n",
    "The algorithm iterates between assigning points to clusters and updating centroids until the centroids do not change significantly or a maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "We will now implement the K-Means clustering algorithm using scikit-learn. To determine the optimal number of clusters, we'll utilize the Elbow Method and the Silhouette Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)  # testing cluster numbers from 2 to 10\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a568f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(K_range, wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4009bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Silhouette Scores\n",
    "plt.plot(K_range, silhouette_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Scores')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above plots, let's assume the optimal number of clusters is 4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(df_scaled)\n",
    "\n",
    "# Append the cluster labels to the original dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis & Visualization\n",
    "\n",
    "We now visualize the clustering results. Below are two key visualizations:\n",
    "\n",
    "1. **Scatter Plot:** Plots 'Annual Income' vs. 'Spending Score' colored by cluster labels.\n",
    "2. **Heatmap:** Shows the average feature values for each cluster (cluster profiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot: Annual Income vs Spending Score\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', \n",
    "                palette='viridis', data=df, legend='full')\n",
    "plt.title('Customer Segmentation (Scatter Plot)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a25975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Cluster Profiles\n",
    "cluster_profile = df.groupby('Cluster').mean()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cluster_profile, annot=True, cmap='coolwarm')\n",
    "plt.title('Cluster Profile Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The clustering results reveal distinct customer segments. For instance:\n",
    "\n",
    "- Some clusters may correspond to high-income customers with a high spending score, indicating premium customers.\n",
    "- Other clusters might represent lower-income customers with different spending behaviors.\n",
    "\n",
    "Potential challenges include:\n",
    "\n",
    "- **Scale Sensitivity:** K-Means is sensitive to the scale of features, making preprocessing crucial.\n",
    "- **Choosing K:** The choice of the number of clusters can greatly affect results.\n",
    "\n",
    "Future improvements could involve:\n",
    "\n",
    "- Experimenting with different distance metrics or clustering algorithms.\n",
    "- Incorporating additional customer features for richer segmentation.\n",
    "- Using dimensionality reduction techniques if additional features are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we applied the K-Means clustering algorithm to perform customer segmentation. Through exploratory analysis, data preprocessing, and model evaluation (using the Elbow Method and Silhouette Score), we identified distinct customer groups. This segmentation can help businesses tailor their marketing and service strategies based on customer profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. MacQueen, J. (1967). _Some Methods for Classification and Analysis of Multivariate Observations_. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability.\n",
    "2. Jain, A. K. (2010). _Data clustering: 50 years beyond K-means_. Pattern Recognition Letters, 31(8), 651-666.\n",
    "3. Pedregosa, F., et al. (2011). _Scikit-learn: Machine Learning in Python_. Journal of Machine Learning Research, 12, 2825-2830.\n",
    "\n",
    "Additional resources and documentation from scikit-learn and relevant data science tutorials were also used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
